{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMQ+gBdDNol/0opTBKFjAFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vldzio/Multimodal-Risk-Profiling/blob/main/US_bankrupt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk27hfJEyNth"
      },
      "outputs": [],
      "source": [
        "#drive link\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from scipy.stats import norm\n",
        "\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
      ],
      "metadata": {
        "id": "KteWHhZRzGp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/USBDT/american_bankruptcy.csv\")"
      ],
      "metadata": {
        "id": "aZmDiniFyduk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "E4CZjlBSy0b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(df):\n",
        "    print(f'data shape: {df.shape}')\n",
        "    summ = pd.DataFrame(df.dtypes, columns=['data type'])\n",
        "    summ['#missing'] = df.isnull().sum().values\n",
        "    summ['%missing'] = df.isnull().sum().values / len(df)* 100\n",
        "    summ['#unique'] = df.nunique().values\n",
        "    desc = pd.DataFrame(df.describe(include='all').transpose())\n",
        "    summ['min'] = desc['min'].values\n",
        "    summ['max'] = desc['max'].values\n",
        "    summ['first value'] = df.loc[0].values\n",
        "    summ['second value'] = df.loc[1].values\n",
        "    summ['third value'] = df.loc[2].values\n",
        "\n",
        "    return summ\n",
        "\n",
        "summary(df)"
      ],
      "metadata": {
        "id": "i1I8MnE0zwDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alive_count = df['status_label'].value_counts()['alive']\n",
        "failed_count = df['status_label'].value_counts()['failed']\n",
        "total_count = alive_count + failed_count\n",
        "alive_ratio = alive_count / total_count\n",
        "failed_ratio = failed_count / total_count\n",
        "\n",
        "print(\"Alive Ratio:\", alive_ratio)\n",
        "print(\"Failed Ratio:\", failed_ratio)"
      ],
      "metadata": {
        "id": "zEbwOtLk5xQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alive_count)\n",
        "print(failed_count)"
      ],
      "metadata": {
        "id": "LRtypnpbS88H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alive_ratio+failed_ratio)"
      ],
      "metadata": {
        "id": "58kdAooUAg8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alive_count = df['status_label'].value_counts()['alive']\n",
        "failed_count = df['status_label'].value_counts()['failed']\n",
        "total_count = alive_count + failed_count\n",
        "alive_ratio = alive_count / total_count\n",
        "failed_ratio = failed_count / total_count\n",
        "\n",
        "labels = ['Alive', 'Failed']\n",
        "sizes = [alive_ratio, failed_ratio]\n",
        "colors = ['green', 'red']\n",
        "explode = (0.1, 0)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.savefig('pie_chart.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1n3-6Sas7EbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/USBDT/\"\n",
        "input_file = folder_path + \"american_bankruptcy.csv\"\n",
        "output_file = folder_path + \"preprocessed_df.csv\"\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "df_latest = df.sort_values(by=['company_name', 'year'], ascending=[True, False]).drop_duplicates(subset=['company_name'], keep='first')\n",
        "\n",
        "df_latest.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Filtered dataset saved as {output_file}\")\n"
      ],
      "metadata": {
        "id": "lVpbQMHECrvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df = pd.read_csv(output_file)\n",
        "preprocessed_df.head()"
      ],
      "metadata": {
        "id": "FuLeu4p_3mKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_mapping = {\n",
        "    \"company_name\": \"Company_Name\",\n",
        "    \"status_label\": \"Status_Label\",\n",
        "    \"year\": \"Year\",\n",
        "    \"X1\": \"Current_Assets\",\n",
        "    \"X2\": \"Cost_of_Goods_Sold\",\n",
        "    \"X3\": \"Depreciation_and_Amortization\",\n",
        "    \"X4\": \"EBITDA\",\n",
        "    \"X5\": \"Inventory\",\n",
        "    \"X6\": \"Net_Income\",\n",
        "    \"X7\": \"Total_Receivables\",\n",
        "    \"X8\": \"Market_Value\",\n",
        "    \"X9\": \"Net_Sales\",\n",
        "    \"X10\": \"Total_Assets\",\n",
        "    \"X11\": \"Total_Long_Term_Debt\",\n",
        "    \"X12\": \"EBIT\",\n",
        "    \"X13\": \"Gross_Profit\",\n",
        "    \"X14\": \"Total_Current_Liabilities\",\n",
        "    \"X15\": \"Retained_Earnings\",\n",
        "    \"X16\": \"Total_Revenue\",\n",
        "    \"X17\": \"Total_Liabilities\",\n",
        "    \"X18\": \"Total_Operating_Expenses\"\n",
        "}\n",
        "\n",
        "preprocessed_df.rename(columns=rename_mapping, inplace=True)\n",
        "\n",
        "print(\"Preprocessed DataFrame has been saved to Google Drive as preprocessed_df.csv.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qnRoo-cj0w8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate ratios for the Altman Z‑Score:\n"
      ],
      "metadata": {
        "id": "B_5CJcA8LAj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Working Capital / Total Assets:\n",
        "preprocessed_df['WC_to_TA'] = (preprocessed_df['Current_Assets'] - preprocessed_df['Total_Current_Liabilities']) / preprocessed_df['Total_Assets']\n",
        "\n",
        "# 2. Retained Earnings / Total Assets:\n",
        "preprocessed_df['RE_to_TA'] = preprocessed_df['Retained_Earnings'] / preprocessed_df['Total_Assets']\n",
        "\n",
        "# 3. EBIT / Total Assets:\n",
        "preprocessed_df['EBIT_to_TA'] = preprocessed_df['EBIT'] / preprocessed_df['Total_Assets']\n",
        "\n",
        "# 4. Market Value of Equity / Total Liabilities:\n",
        "preprocessed_df['MVE_to_TL'] = preprocessed_df['Market_Value'] / preprocessed_df['Total_Liabilities']\n",
        "\n",
        "# 5. Net Sales / Total Assets:\n",
        "preprocessed_df['Sales_to_TA'] = preprocessed_df['Net_Sales'] / preprocessed_df['Total_Assets']\n"
      ],
      "metadata": {
        "id": "P6HA4Jy15qpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate ratios for the Zmijewski Model:\n"
      ],
      "metadata": {
        "id": "89GD-uCULEvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Return on Assets (ROA) = Net Income / Total Assets\n",
        "preprocessed_df['ROA'] = preprocessed_df['Net_Income'] / preprocessed_df['Total_Assets']\n",
        "\n",
        "# 7. Leverage = Total Liabilities / Total Assets\n",
        "preprocessed_df['Leverage'] = preprocessed_df['Total_Liabilities'] / preprocessed_df['Total_Assets']\n",
        "\n",
        "# 8. Liquidity (using the Current Ratio):\n",
        "preprocessed_df['Current_Ratio_Calc'] = preprocessed_df['Current_Assets'] / preprocessed_df['Total_Current_Liabilities']\n"
      ],
      "metadata": {
        "id": "JVRI6nhM8gbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate ratios for the Ohlson O‑Score:\n"
      ],
      "metadata": {
        "id": "9mq0gjBOLGUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['ln_TA'] = np.log(preprocessed_df['Total_Assets'])\n",
        "preprocessed_df['CL_to_CA'] = preprocessed_df['Total_Current_Liabilities'] / preprocessed_df['Current_Assets']\n",
        "preprocessed_df['Neg_Net_Income'] = (preprocessed_df['Net_Income'] < 0).astype(int)"
      ],
      "metadata": {
        "id": "Sh2lhdwF8jZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df.to_csv('/content/drive/My Drive/USBDT/preprocessed_df.csv', index=False)\n",
        "\n",
        "print(\"Updated preprocessed_df.csv with computed ratios has been saved to Google Drive.\")"
      ],
      "metadata": {
        "id": "aocTQDsw-RNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "verification"
      ],
      "metadata": {
        "id": "q-CmL10JLYD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_df[['Company_Name', 'WC_to_TA', 'RE_to_TA', 'EBIT_to_TA',\n",
        "                       'MVE_to_TL', 'Sales_to_TA', 'ROA', 'Leverage', 'Current_Ratio_Calc',\n",
        "                       'ln_TA', 'Neg_Net_Income']].head())"
      ],
      "metadata": {
        "id": "aKzDwKEQ8j10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/preprocessed_df.csv')"
      ],
      "metadata": {
        "id": "KksvrdOy9O_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the Altman Z‑Score\n",
        "Formula: Z = 1.2*(WC_to_TA) + 1.4*(RE_to_TA) + 3.3*(EBIT_to_TA) + 0.6*(MVE_to_TL) + 1.0*(Sales_to_TA)"
      ],
      "metadata": {
        "id": "D6-j-QlQLbtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['Altman_Z'] = (1.2 * preprocessed_df['WC_to_TA'] +\n",
        "                  1.4 * preprocessed_df['RE_to_TA'] +\n",
        "                  3.3 * preprocessed_df['EBIT_to_TA'] +\n",
        "                  0.6 * preprocessed_df['MVE_to_TL'] +\n",
        "                  1.0 * preprocessed_df['Sales_to_TA'])"
      ],
      "metadata": {
        "id": "nHLND9Mt9UsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Assigning risk categories for the Altman Z‑Score (using common thresholds):\n",
        "   Z > 2.99   => Safe (Low risk)\n",
        "   1.81 < Z <= 2.99 => Gray Zone (Moderate risk)\n",
        "   Z <= 1.81  => Distress (High risk)"
      ],
      "metadata": {
        "id": "QglaWa4xLgS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def altman_risk(z):\n",
        "    if z > 2.99:\n",
        "        return 'Safe'\n",
        "    elif z > 1.81:\n",
        "        return 'Gray'\n",
        "    else:\n",
        "        return 'Distress'\n",
        "\n",
        "preprocessed_df['Altman_Risk'] = preprocessed_df['Altman_Z'].apply(altman_risk)"
      ],
      "metadata": {
        "id": "-_asRTRDLeY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the Zmijewski Score\n",
        "Zmijewski_Score = -4.3 - 4.5 * ROA + 5.7 * Leverage - 0.004 * Current_Ratio_Calc"
      ],
      "metadata": {
        "id": "a9WxD7QpLlmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['Zmijewski_Score'] = (-4.3\n",
        "                         - 4.5 * preprocessed_df['ROA']\n",
        "                         + 5.7 * preprocessed_df['Leverage']\n",
        "                         - 0.004 * preprocessed_df['Current_Ratio_Calc'])"
      ],
      "metadata": {
        "id": "XKbAw_9d-enP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the Zmijewski Score into a bankruptcy probability using the standard normal CDF:\n",
        "If Zmijewski_Prob > 0.5, classify as \"High Risk\"\n",
        "Otherwise, classify as \"Low Risk\""
      ],
      "metadata": {
        "id": "Zf9P2TZuL1OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['Zmijewski_Prob'] = norm.cdf(preprocessed_df['Zmijewski_Score'])\n",
        "\n",
        "\n",
        "preprocessed_df['Zmijewski_Risk'] = preprocessed_df['Zmijewski_Prob'].apply(lambda p: 'High Risk' if p > 0.5 else 'Low Risk')"
      ],
      "metadata": {
        "id": "zxi-ZghLL0IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ohlson O‑Score\n",
        "O = -1.32 - 0.407*ln_TA + 6.03*(Total_Liabilities/Total_Assets)\n",
        " - 1.43*(WC_to_TA) + 0.0757*(CL_to_CA) - 2.37*(ROA) - 1.83*(FFOI) + 0.285*(Neg_Net_Income)"
      ],
      "metadata": {
        "id": "wdlq7phXL7ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preprocessed_df['Ohlson_Score'] = (-1.32\n",
        "                      - 0.407 * preprocessed_df['ln_TA']\n",
        "                      + 6.03 * (preprocessed_df['Total_Liabilities'] / preprocessed_df['Total_Assets'])\n",
        "                      - 1.43 * preprocessed_df['WC_to_TA']\n",
        "                      + 0.0757 * preprocessed_df['CL_to_CA']\n",
        "                      - 2.37 * (preprocessed_df['Net_Income'] / preprocessed_df['Total_Assets'])\n",
        "                      + 0.285 * preprocessed_df['Neg_Net_Income'])"
      ],
      "metadata": {
        "id": "h-MjDaie_vVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the Ohlson score to a bankruptcy probability using the logistic function:\n",
        "Classify as 'High Risk' if probability > 0.5, else 'Low Risk"
      ],
      "metadata": {
        "id": "myAg4uZwMDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['Ohlson_Prob'] = 1 / (1 + np.exp(-preprocessed_df['Ohlson_Score']))\n",
        "\n",
        "preprocessed_df['Ohlson_Risk'] = preprocessed_df['Ohlson_Prob'].apply(lambda p: 'High Risk' if p > 0.5 else 'Low Risk')"
      ],
      "metadata": {
        "id": "5HmUhLgQMIYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining risk profile metrics\n",
        "\n",
        "If Altman is in Distress or either Zmijewski or Ohlson indicate High Risk, classify as High Risk.\n"
      ],
      "metadata": {
        "id": "Egu94j9J-m1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overall_risk(altman, zmijewski, ohlson):\n",
        "    if altman == 'Distress' or zmijewski == 'High Risk' or ohlson == 'High Risk':\n",
        "        return 'Risk'\n",
        "    elif altman == 'Gray':\n",
        "        return 'Risk'\n",
        "    else:\n",
        "        return 'Low Risk'\n",
        "\n",
        "preprocessed_df['Overall_Risk'] = preprocessed_df.apply(lambda row: overall_risk(row['Altman_Risk'], row['Zmijewski_Risk'], row['Ohlson_Risk']), axis=1)"
      ],
      "metadata": {
        "id": "FyavGd0C-jUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving risk profiles"
      ],
      "metadata": {
        "id": "fM8d8VTsBCC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df.to_csv('/content/drive/My Drive/USBDT/preprocessed_df.csv', index=False)\n",
        "print(\"Risk profile values have been calculated and saved to preprocessed_df.csv.\")"
      ],
      "metadata": {
        "id": "kXKFp3i4Ao7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "overall risk metric to bankruptcy accuracy check\n",
        "\n",
        "Convert overall risk into binary predictions:\n",
        "\n",
        "We'll assume that if Overall_Risk is \"High Risk\", then we predict 'failed',\n",
        "\n",
        "otherwise we predict 'alive'."
      ],
      "metadata": {
        "id": "zMwdEuceFTU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df.head(), df.shape"
      ],
      "metadata": {
        "id": "G7DIAHEWnSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_df['Status_Label'].value_counts().get('alive', 0), preprocessed_df['Status_Label'].value_counts().get('failed', 0)"
      ],
      "metadata": {
        "id": "qwSScKXUT-mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Imbalance Handling\n",
        "##random undersampling"
      ],
      "metadata": {
        "id": "KxkO38_gmcBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "l_nHgnHUe2IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/USBDT/preprocessed_df.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "alive_df = df[df['Status_Label'] == 'alive']\n",
        "failed_df = df[df['Status_Label'] == 'failed']\n",
        "\n",
        "alive_sampled = alive_df.sample(n=len(failed_df), random_state=42)  # Downsample \"alive\" to match \"failed\"\n",
        "undersampled_df = pd.concat([alive_sampled, failed_df])\n",
        "\n",
        "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "undersampled_file_path = '/content/drive/My Drive/USBDT/undersampled_df.csv'\n",
        "undersampled_df.to_csv(undersampled_file_path, index=False)\n",
        "\n",
        "print(undersampled_df['Status_Label'].value_counts())"
      ],
      "metadata": {
        "id": "qfTKKRCNXiA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pyEYh4hAXhSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "file_path = \"/content/drive/My Drive/USBDT/undersampled_df.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df = df.drop(columns=['Company_Name', 'Year', 'Altman_Risk', 'Zmijewski_Risk', 'Ohlson_Risk', 'Overall_Risk'])\n",
        "\n",
        "X = df.drop(columns=['Status_Label'])  # Features\n",
        "y = df['Status_Label']                 # Target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_normalized = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "X_normalized['Status_Label'] = y.reset_index(drop=True)\n",
        "\n",
        "X_normalized.to_csv(\"/content/drive/My Drive/USBDT/undersampled_normalized_df.csv\", index=False)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_normalized.drop(columns=['Status_Label']),\n",
        "    X_normalized['Status_Label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=X_normalized['Status_Label']\n",
        ")\n",
        "\n",
        "print(f\"Training Set: {X_train.shape}, Test Set: {X_test.shape}\")\n",
        "print(\"✅ Normalized undersampled dataset saved.\")\n"
      ],
      "metadata": {
        "id": "fq3pYuj_gHJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression on Undersampled dataset"
      ],
      "metadata": {
        "id": "I6v9LDzoPFl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "lr = LogisticRegression(max_iter = 100)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "y_prob = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'Logistic Regression (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V9Uc2uwpOZja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest on undersampled dataset"
      ],
      "metadata": {
        "id": "Wt1AzRnVPCSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]  # Probabilities for ROC curve\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'Random Forest (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Random Forest\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P4c86lgJOuCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###XGBoost on Undersample"
      ],
      "metadata": {
        "id": "8F_9b5fAPKdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "y_train_numeric = y_train.map({'alive': 0, 'failed': 1})\n",
        "y_test_numeric = y_test.map({'alive': 0, 'failed': 1})\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train_numeric)\n",
        "\n",
        "y_pred = xgb.predict(X_test)\n",
        "y_prob = xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
        "precision = precision_score(y_test_numeric, y_pred)\n",
        "recall = recall_score(y_test_numeric, y_pred)\n",
        "f1 = f1_score(y_test_numeric, y_pred)\n",
        "roc_auc = roc_auc_score(y_test_numeric, y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test_numeric, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - XGBoost\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test_numeric, y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'XGBoost (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - XGBoost\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eq8M9fVSPPJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVM with undersampling"
      ],
      "metadata": {
        "id": "QBTmmSUGBb8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "y_train_numeric = y_train.map({'alive': 0, 'failed': 1})\n",
        "y_test_numeric = y_test.map({'alive': 0, 'failed': 1})\n",
        "\n",
        "svm = SVC(probability=True)\n",
        "svm.fit(X_train, y_train_numeric)\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "y_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test_numeric, y_pred)\n",
        "precision = precision_score(y_test_numeric, y_pred)\n",
        "recall = recall_score(y_test_numeric, y_pred)\n",
        "f1 = f1_score(y_test_numeric, y_pred)\n",
        "roc_auc = roc_auc_score(y_test_numeric, y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test_numeric, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - SVM\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test_numeric, y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'SVM (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - SVM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wCqVQauBBews"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN with undersampling"
      ],
      "metadata": {
        "id": "PNpjrV9jBi8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - KNN\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'KNN (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - KNN\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7M84FzcuBiO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using smote on the same dataset"
      ],
      "metadata": {
        "id": "tPi3JfL7GEXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "file_path = \"/content/drive/My Drive/USBDT/preprocessed_df.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "df = df.drop(columns=['Company_Name', 'Year', 'Altman_Risk', 'Zmijewski_Risk', 'Ohlson_Risk', 'Overall_Risk'])\n",
        "\n",
        "X = df.drop(columns=['Status_Label'])\n",
        "y = df['Status_Label']\n",
        "\n",
        "y_numeric = y.map({'alive': 0, 'failed': 1})\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y_numeric)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
        "\n",
        "X_resampled_normalized = pd.DataFrame(X_resampled_scaled, columns=X.columns)\n",
        "\n",
        "y_resampled = pd.Series(y_resampled).map({0: 'alive', 1: 'failed'})\n",
        "\n",
        "oversampled_normalized_df = pd.concat([X_resampled_normalized, y_resampled.rename(\"Status_Label\")], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled_normalized,\n",
        "    y_resampled,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_resampled\n",
        ")\n",
        "\n",
        "\n",
        "oversampled_path = \"/content/drive/My Drive/USBDT/oversampled_normalized_df.csv\"\n",
        "oversampled_normalized_df.to_csv(oversampled_path, index=False)\n",
        "\n",
        "print(f\"✅ Normalized oversampled dataset saved at: {oversampled_path}\")\n"
      ],
      "metadata": {
        "id": "B9pSlIPdGG_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KxR0s2peGStr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression on SMOTE"
      ],
      "metadata": {
        "id": "0zuFZKzucheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(X_test)\n",
        "y_prob = lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'Logistic Regression (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plnmY4vqGI4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest on SMOTE"
      ],
      "metadata": {
        "id": "W5Zh1v16cken"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "y_prob = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'Random Forest (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - Random Forest\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C1nMrjMncmyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost on SMOTE"
      ],
      "metadata": {
        "id": "e95Q-Mo3coIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train.map({'alive': 0, 'failed': 1}))\n",
        "\n",
        "y_pred = xgb.predict(X_test)\n",
        "y_prob = xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test.map({'alive': 0, 'failed': 1}), y_pred)\n",
        "precision = precision_score(y_test.map({'alive': 0, 'failed': 1}), y_pred)\n",
        "recall = recall_score(y_test.map({'alive': 0, 'failed': 1}), y_pred)\n",
        "f1 = f1_score(y_test.map({'alive': 0, 'failed': 1}), y_pred)\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test.map({'alive': 0, 'failed': 1}), y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - XGBoost\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'XGBoost (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - XGBoost\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZIw_oAoPcrjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM on SMOTE"
      ],
      "metadata": {
        "id": "uDqbRX6Xctwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "svm = SVC(probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "y_prob = svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - SVM\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'SVM (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - SVM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9s6vYpZBcu6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN on SMOTE"
      ],
      "metadata": {
        "id": "5_stj1j1cwu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"failed\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"failed\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"failed\")\n",
        "roc_auc = roc_auc_score(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\", xticklabels=['alive', 'failed'], yticklabels=['alive', 'failed'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - KNN\")\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test.map({'alive': 0, 'failed': 1}), y_prob)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'KNN (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve - KNN\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zBFGb7sAcyou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "rtm1m3nRC4zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "cCaHaTWsLo-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cVI0CBq4Lq5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return sentiment_map[predicted_class]\n"
      ],
      "metadata": {
        "id": "-Oe6sL18LsfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_predict(texts):\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predicted_classes = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    return [sentiment_map[label] for label in predicted_classes]\n"
      ],
      "metadata": {
        "id": "wGq0tG1zLvN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"The company's earnings report exceeded expectations, driving stock prices up.\"\n",
        "print(\"Sentiment:\", predict_sentiment(sample_text))\n",
        "\n",
        "financial_texts = [\n",
        "    \"The market is expected to see a major downturn following weak economic data.\",\n",
        "    \"Company profits surged 15% last quarter.\",\n",
        "    \"Investors remain cautious as geopolitical risks increase.\"\n",
        "]\n",
        "print(\"Batch Sentiments:\", batch_predict(financial_texts))\n"
      ],
      "metadata": {
        "id": "yIMWy-C0LxO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Phrasebank/phrasebank_data.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "A-CJ059eLykY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
        "df['Sentiment'] = df['Sentiment'].map(label_map)\n",
        "\n",
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\$\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df['Sentence'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['Sentence'].tolist(),\n",
        "    df['Sentiment'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "zx6wOOjpNB9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "qf13Oo2zN_-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': train_labels\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'input_ids': val_encodings['input_ids'],\n",
        "    'attention_mask': val_encodings['attention_mask'],\n",
        "    'labels': val_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "b9I2xHjTNufa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "5664V7NPN3q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "eLYvlzMXSn8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate\n"
      ],
      "metadata": {
        "id": "RL12YM8BUMuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    f1_score = f1_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    return {\"f1_score\": f1_score['f1']}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finbert_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=8e-6,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "dXBJ9m9lSorS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}